# -*- coding: utf-8 -*-
"""hierarchical-cluster.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1I-WEXssgTspMNvOUNci5BdRESsOzZ6B9

# About this Dataset

Context
This dataset contains an airline passenger satisfaction survey. What factors are highly correlated to a satisfied (or dissatisfied) passenger? Can you predict passenger satisfaction?

Content
Gender: Gender of the passengers (Female, Male)

Customer Type: The customer type (Loyal customer, disloyal customer)

Age: The actual age of the passengers

Type of Travel: Purpose of the flight of the passengers (Personal Travel, Business Travel)

Class: Travel class in the plane of the passengers (Business, Eco, Eco Plus)

Flight distance: The flight distance of this journey

Inflight wifi service: Satisfaction level of the inflight wifi service (0:Not Applicable;1-5)

Departure/Arrival time convenient: Satisfaction level of Departure/Arrival time convenient

Ease of Online booking: Satisfaction level of online booking

Gate location: Satisfaction level of Gate location

Food and drink: Satisfaction level of Food and drink

Online boarding: Satisfaction level of online boarding

Seat comfort: Satisfaction level of Seat comfort

Inflight entertainment: Satisfaction level of inflight entertainment

On-board service: Satisfaction level of On-board service

Leg room service: Satisfaction level of Leg room service

Baggage handling: Satisfaction level of baggage handling

Check-in service: Satisfaction level of Check-in service

Inflight service: Satisfaction level of inflight service

Cleanliness: Satisfaction level of Cleanliness

Departure Delay in Minutes: Minutes delayed when departure

Arrival Delay in Minutes: Minutes delayed when Arrival

Satisfaction: Airline satisfaction level(Satisfaction, neutral or dissatisfaction)

# step1 Data Collection

In this step, we import Libraries and load the  dataset We will first inspect the dataset to understand its structure.
"""

#import Libraries

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

#Load Dataset
train=pd.read_csv("/content/train.csv")
test=pd.read_csv("/content/test.csv")

print(train.shape, test.shape)

# merge train and test together
data = pd.concat([train, test], axis=0)

print(data.shape)
# display 5 rows
data.head()

data.info()##show num of row , col ,datatype and missing value

data.describe() # show missing value , mean , std ( probablity)

"""# 2.Data Exploration

Before preprocessing, it's important to explore the data. if there are any missing values and check outlier and visualization
"""

#drop  2 col

data.drop(columns=["Unnamed: 0", "id"], inplace=True)
data.dropna(inplace=True)

data.head()

data.isnull().sum()

"""# Data Preprocessing

we Encode categorical features , feature scaling ,Outlier Detection and Dimensionality Reduction
"""

# convert data categorical into (encoding)
from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()
for col in ["Gender", "Customer Type", "Type of Travel", "Class", "satisfaction"]:
    data[col] = le.fit_transform(data[col])

"""feature scaling"""

features = data.drop(columns=["satisfaction"]) # drop target label col

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_scaled = scaler.fit_transform(features)

scaled_df = pd.DataFrame(X_scaled, columns=features.columns)

"""check outlier"""

from scipy import stats

z_scores = np.abs(stats.zscore(scaled_df))
outliers = (z_scores > 3)
outlier_counts = pd.Series(outliers.sum(axis=0), index=scaled_df.columns)
print("Number of outliers in each column:")
print(outlier_counts)

col = "Age"

plt.figure(figsize=(14,6))

# Scatter plot before cleaning
plt.subplot(2,2,1)
plt.scatter(range(len(scaled_df[col])), scaled_df[col], alpha=0.5)
plt.title(f"{col} - of outlier")
plt.ylabel(col)

col = "Flight Distance"

plt.figure(figsize=(14,6))

# Scatter plot before cleaning
plt.subplot(2,2,1)
plt.scatter(range(len(scaled_df[col])), scaled_df[col], alpha=0.5)
plt.title(f"{col} -of outlier")
plt.ylabel(col)

col = "Arrival Delay in Minutes"

plt.figure(figsize=(14,6))

# Scatter plot before cleaning
plt.subplot(2,2,1)
plt.scatter(range(len(scaled_df[col])), scaled_df[col], alpha=0.5)
plt.title(f"{col} -of outlier")
plt.ylabel(col)

col = "Flight Distance"

plt.figure(figsize=(14,6))

# Scatter plot before cleaning
plt.subplot(2,2,1)
plt.scatter(range(len(scaled_df[col])), scaled_df[col], alpha=0.5)
plt.title(f"{col} -of outlier")
plt.ylabel(col)

def clean_outliers_iqr(df, column):
    Q1 = df[column].quantile(0.25)
    Q3 = df[column].quantile(0.75)
    IQR = Q3 - Q1

    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR


    df[column] = np.where(df[column] < lower_bound, lower_bound, df[column])
    df[column] = np.where(df[column] > upper_bound, upper_bound, df[column])
    return df

# col of outlier
cols_with_outliers = ["Age", "Flight Distance", "Inflight service",
                      "Departure Delay in Minutes", "Arrival Delay in Minutes"]

# clean oulier of cols
for col in cols_with_outliers:
    scaled_df = clean_outliers_iqr(scaled_df, col)

print(" Outliers cleand ")

col = "Flight Distance"

plt.figure(figsize=(14,6))
plt.scatter(range(len(scaled_df[col])), scaled_df[col], alpha=0.5, color="green")
plt.title(f"{col} - After Cleaning (Scatter)")
plt.ylabel(col)

"""# Apply Hierarchical Clustering"""

from scipy.cluster.hierarchy import dendrogram, linkage
import matplotlib.pyplot as plt

Z = linkage(scaled_df, method="ward")  # ward / complete / average

# Plot dendrogram
plt.figure(figsize=(12, 6))
dendrogram(Z, truncate_mode="level", p=5)
plt.title("Hierarchical Clustering Dendrogram")
plt.xlabel("Samples")
plt.ylabel("Distance")
plt.show()

#Choose number of clusters
from sklearn.cluster import AgglomerativeClustering

model = AgglomerativeClustering(n_clusters=4, linkage="ward")
labels = model.fit_predict(scaled_df)
data["Cluster"] = labels

from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score

sil_score = silhouette_score(scaled_df, labels)

print(" Evaluation Results:")
print(f"Silhouette Score: {sil_score:.3f}")

for k in range(2, 10):
    model = AgglomerativeClustering(n_clusters=k, linkage="ward")
    labels = model.fit_predict(scaled_df)
    sil = silhouette_score(scaled_df, labels)
    print(f"Clusters: {k}, Silhouette: {sil:.3f}")

best_k = 3
model = AgglomerativeClustering(n_clusters=best_k, linkage='ward')
labels = model.fit_predict(scaled_df)

import seaborn as sns
import matplotlib.pyplot as plt

corr_matrix = scaled_df.corr()

# Heatmap
plt.figure(figsize=(12,8))
sns.heatmap(corr_matrix, annot=False, cmap="coolwarm", center=0)
plt.title("Feature Correlation Matrix")
plt.show()

#
drop_features = [
    "Gender",
    "Age",
    "Gate location",
    "Departure/Arrival time convenient",
    "Departure Delay in Minutes"
]


df_selected = data.drop(columns=drop_features)

print("Remaining features after selection:")
print(df_selected.columns.tolist())

for k in range(2, 10):
    model = AgglomerativeClustering(n_clusters=k, linkage="ward")
    labels = model.fit_predict(df_selected)
    sil = silhouette_score(df_selected, labels)
    print(f"Clusters: {k}, Silhouette: {sil:.3f}")

#Choose number of clusters
from sklearn.cluster import AgglomerativeClustering

model = AgglomerativeClustering(n_clusters=2, linkage="ward")
labels = model.fit_predict(df_selected)
data["Cluster"] = labels

sil_score = silhouette_score(df_selected, labels)

print(" Evaluation Results:")
print(f"Silhouette Score: {sil_score:.3f}")

